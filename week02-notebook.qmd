---
title: "Week 02: SQL â€“ Intermediate Concepts Summary"
format:
  html:
    code-fold: false
jupyter: python3
---

## SLOs for Week 02

At the end of this unit, students will be able to...

1.  Perform data aggregation using GROUP BY and HAVING.
2.  Write and execute INNER, LEFT-OUTER, RIGHT, and FULL JOINs.
3.  Construct subqueries for complex queries.
4.  Implement SQL functions and expressions.
5.  Use SQL best practices for readable and efficient queries.
6.  Optimize SQL queries for better performance.

## 0. Creating the Library Survey Tables

In the US, the Institute of Museum and Library Services (IMLS) measures library activity as part of its annual Public Libraries Survey. The survey collects data from more than 9000 library administrative entities, defined by the survey as agencies that provide library services to a particular locality. Data includes the number of branches, staff, books, hours open per year, etc. To teach the concepts below, we will build three tables containing the data from the survey related to the years of 2016, 2017 and 2018. For doing so, we read from the CSV files downloaded from their website. More especifically, some columns will be selected in the process to reduce the amount of non used attributes.

We are running SQL queries in a Jupyter environment. 

```{python}
%%capture
%load_ext sql
%sql sqlite:///dbs/w02/library.db
%config SqlMagic.style = '_DEPRECATED_DEFAULT'
```

---

**This will** open our database `library.db` for us. Don't bother yourself with the config details. That is a trick to run the SQL queries in this environment.

Now we turn to our tables. We have to create 3 tables. Let's start with the table for 2018.

```{python}
#| label: create-2018
%%sql
-- Creating the 2018 Public Libraries Survey table

-- We drop an old copy of the table, if it exists.
DROP TABLE IF EXISTS libraries_2018;

CREATE TABLE libraries_2018 (
    stabr text NOT NULL,
    fscskey text CONSTRAINT fscskey_2018_pkey PRIMARY KEY,
    libid text NOT NULL,
    libname text NOT NULL,
    address text NOT NULL,
    city text NOT NULL,
    zip text NOT NULL,
    county text NOT NULL,
    phone text NOT NULL,
    c_relatn text NOT NULL,
    c_legbas text NOT NULL,
    c_admin text NOT NULL,
    c_fscs text NOT NULL,
    geocode text NOT NULL,
    lsabound text NOT NULL,
    startdate text NOT NULL,
    enddate text NOT NULL,
    popu_lsa integer NOT NULL,
    popu_und integer NOT NULL,
    centlib integer NOT NULL,
    branlib integer NOT NULL,
    bkmob integer NOT NULL,
    totstaff numeric(8,2) NOT NULL,
    bkvol integer NOT NULL,
    ebook integer NOT NULL,
    audio_ph integer NOT NULL,
    audio_dl integer NOT NULL,
    video_ph integer NOT NULL,
    video_dl integer NOT NULL,
    ec_lo_ot integer NOT NULL,
    subscrip integer NOT NULL,
    hrs_open integer NOT NULL,
    visits integer NOT NULL,
    reference integer NOT NULL,
    regbor integer NOT NULL,
    totcir integer NOT NULL,
    kidcircl integer NOT NULL,
    totpro integer NOT NULL,
    gpterms integer NOT NULL,
    pitusr integer NOT NULL,
    wifisess integer NOT NULL,
    obereg text NOT NULL,
    statstru text NOT NULL,
    statname text NOT NULL,
    stataddr text NOT NULL,
    longitude numeric(10,7) NOT NULL,
    latitude numeric(10,7) NOT NULL
);
```

---

**This is** an empty table. To fill the table, we need to convert our CSV entries to entitites in our database.

Here I came up with another trick. I created a subprocess that opens SQLite3 and calls the command to convert a CSV file into a table inside the database. The code is as follows:

```{python}
import subprocess

db = "dbs/w02/library.db"
csv = "dbs/w02/pls_fy2018_libraries.csv"
table = "libraries_2018"

sqlite_command = f"""
.mode csv
.import '{csv}' {table}
"""

subprocess.run(["sqlite3", db], input=sqlite_command, text=True)
```

---

**Let's see** if the data was loaded? Let's peak on the 10 first rows.

```{python}
%%sql
SELECT * FROM libraries_2018
LIMIT 10;
```

---

**Now, lets** count how many rows are in this table.

```{python}
%%sql
SELECT count(*) FROM libraries_2018;
```

---

**The same** should be done for the years 2016 and 2017. The code below is long, but identical to what we've done above.

```{python}
#| label: create-2016
#| code-fold: true
%%sql
-- Creating the 2016 Public Libraries Survey table

-- We drop an old copy of the table, if it exists.
DROP TABLE IF EXISTS libraries_2016;
DROP TABLE IF EXISTS libraries_2017;

CREATE TABLE libraries_2016 (
    stabr text NOT NULL,
    fscskey text CONSTRAINT fscskey_2018_pkey PRIMARY KEY,
    libid text NOT NULL,
    libname text NOT NULL,
    address text NOT NULL,
    city text NOT NULL,
    zip text NOT NULL,
    county text NOT NULL,
    phone text NOT NULL,
    c_relatn text NOT NULL,
    c_legbas text NOT NULL,
    c_admin text NOT NULL,
    c_fscs text NOT NULL,
    geocode text NOT NULL,
    lsabound text NOT NULL,
    startdate text NOT NULL,
    enddate text NOT NULL,
    popu_lsa integer NOT NULL,
    popu_und integer NOT NULL,
    centlib integer NOT NULL,
    branlib integer NOT NULL,
    bkmob integer NOT NULL,
    totstaff numeric(8,2) NOT NULL,
    bkvol integer NOT NULL,
    ebook integer NOT NULL,
    audio_ph integer NOT NULL,
    audio_dl integer NOT NULL,
    video_ph integer NOT NULL,
    video_dl integer NOT NULL,
    ec_lo_ot integer NOT NULL,
    subscrip integer NOT NULL,
    hrs_open integer NOT NULL,
    visits integer NOT NULL,
    reference integer NOT NULL,
    regbor integer NOT NULL,
    totcir integer NOT NULL,
    kidcircl integer NOT NULL,
    totpro integer NOT NULL,
    gpterms integer NOT NULL,
    pitusr integer NOT NULL,
    wifisess integer NOT NULL,
    obereg text NOT NULL,
    statstru text NOT NULL,
    statname text NOT NULL,
    stataddr text NOT NULL,
    longitude numeric(10,7) NOT NULL,
    latitude numeric(10,7) NOT NULL
);

CREATE TABLE libraries_2017 (
    stabr text NOT NULL,
    fscskey text CONSTRAINT fscskey_2018_pkey PRIMARY KEY,
    libid text NOT NULL,
    libname text NOT NULL,
    address text NOT NULL,
    city text NOT NULL,
    zip text NOT NULL,
    county text NOT NULL,
    phone text NOT NULL,
    c_relatn text NOT NULL,
    c_legbas text NOT NULL,
    c_admin text NOT NULL,
    c_fscs text NOT NULL,
    geocode text NOT NULL,
    lsabound text NOT NULL,
    startdate text NOT NULL,
    enddate text NOT NULL,
    popu_lsa integer NOT NULL,
    popu_und integer NOT NULL,
    centlib integer NOT NULL,
    branlib integer NOT NULL,
    bkmob integer NOT NULL,
    totstaff numeric(8,2) NOT NULL,
    bkvol integer NOT NULL,
    ebook integer NOT NULL,
    audio_ph integer NOT NULL,
    audio_dl integer NOT NULL,
    video_ph integer NOT NULL,
    video_dl integer NOT NULL,
    ec_lo_ot integer NOT NULL,
    subscrip integer NOT NULL,
    hrs_open integer NOT NULL,
    visits integer NOT NULL,
    reference integer NOT NULL,
    regbor integer NOT NULL,
    totcir integer NOT NULL,
    kidcircl integer NOT NULL,
    totpro integer NOT NULL,
    gpterms integer NOT NULL,
    pitusr integer NOT NULL,
    wifisess integer NOT NULL,
    obereg text NOT NULL,
    statstru text NOT NULL,
    statname text NOT NULL,
    stataddr text NOT NULL,
    longitude numeric(10,7) NOT NULL,
    latitude numeric(10,7) NOT NULL
);
```

---

**And now**, let's fill the tables with data from our CSV files.

```{python}
#| code-fold: true
# 2016

db = "dbs/w02/library.db"
csv = "dbs/w02/pls_fy2016_libraries.csv"
table = "libraries_2016"

sqlite_command = f"""
.mode csv
.import '{csv}' {table}
"""
subprocess.run(["sqlite3", db], input=sqlite_command, text=True)

# 2017

db = "dbs/w02/library.db"
csv = "dbs/w02/pls_fy2017_libraries.csv"
table = "libraries_2017"

sqlite_command = f"""
.mode csv
.import '{csv}' {table}
"""
subprocess.run(["sqlite3", db], input=sqlite_command, text=True)


```

---

**Let's check** the rows count for each of the tables now.

```{python}
%%sql
    SELECT 2016 AS 'Year', count(*) AS NumRows FROM libraries_2016
UNION ALL
    SELECT 2017, count(*) AS NumRows FROM libraries_2017
UNION ALL
    SELECT 2018, count(*) AS NumRows FROM libraries_2018;
```

---

## ðŸ§® 1. SQL Aggregation & Grouping (with SQLite Notes)

SQL lets you **summarize** data using **aggregate functions**, and **group** it with GROUP BY. This is especially helpful for statistics, reports, and dashboards.

### ðŸŽ¯ **Common Aggregate Functions**

| Function    | Description          | Example              |
|-------------|----------------------|----------------------|
| `COUNT()`   | Number of rows       | `COUNT(*)`           |
| `SUM()`     | Total of values      | `SUM(amount)`        |
| `AVG()`     | Mean average         | `AVG(score)`         |
| `MIN()`     | Smallest value       | `MIN(age)`           |
| `MAX()`     | Largest value        | `MAX(salary)`        |

:::{.callout-note}
âœ… **SQLite supports all these functions.**
:::

### ðŸ§© Basic Grouping Example

**Q:** How many students are in each major?

```sql
SELECT major, COUNT(*) AS student_count
FROM students
GROUP BY major;
```

:::{.callout-note}
âœ… SQLite allows this even if youâ€™re selecting columns not in the GROUP BY clause or not inside an aggregate function. It will return one arbitrary value from each group for such columns.
:::

**Example (SQLite accepts this):**

```sql
SELECT major, name
FROM students
GROUP BY major;
```

:::{.callout-note}
This works in SQLite â€” it will return one name for each major, but which name is returned is undefined.
âŒ PostgreSQL (and other standards-compliant SQL engines) will **reject** this query with an error unless name is also in the GROUP BY or wrapped in an aggregate like MIN(name).
:::

### ðŸ” Filtering Groups with HAVING

Use HAVING to filter after grouping.

**Q:** Show only majors with more than 10 students:

```sql
SELECT major, COUNT(*) AS student_count
FROM students
GROUP BY major
HAVING COUNT(*) > 10;
```

:::{.callout-note}
â„¹ï¸ In SQLite, you can refer to the alias (student_count) in HAVING.
In PostgreSQL, youâ€™d need to repeat the expression: HAVING COUNT(*) > 10.
:::

### ðŸ“Š Combining Aggregates

**Q:** Show average GPA per department â€” only if itâ€™s above 3.5:

```sql
SELECT department, AVG(gpa) AS avg_gpa
FROM students
GROUP BY department
HAVING AVG(gpa) > 3.5;
```

### ðŸ”— Grouping by Expressions

You can group by computed values like substrings or date parts.

**Q:** Count students by **admission year** (from text-based dates):

```sql
SELECT SUBSTR(admission_date, 1, 4) AS year, COUNT(*) AS num_students
FROM students
GROUP BY year;
```

The SUBSTR() function extracts a **substring** from a given string.

```sql
SUBSTR(string, start, length)
```
* string: the text to extract from
* start: the position to start (1-based index)
* length (optional): how many characters to return

```sql
-- Get the first 4 characters of a date
SELECT SUBSTR('2024-09-15', 1, 4);  -- Returns '2024'

-- Get the first letter of a name
SELECT SUBSTR(name, 1, 1) FROM students;
```

ðŸ“Œ If length is omitted, SQLite returns the rest of the string from start.

:::{.callout-note}
âœ… Works well in SQLite (which stores dates as text: YYYY-MM-DD).
â— In PostgreSQL, use: EXTRACT(YEAR FROM admission_date).
:::

### ðŸ§  Tips for Grouping Queries

* Use GROUP BY with aggregate functions.
* Use HAVING to filter groups (not rows).
* Use WHERE for filtering before grouping.
* Prefer column names or expressions in GROUP BY, not just positions (e.g., avoid GROUP BY 1).
* Be cautious using non-aggregated, non-grouped columns in SQLite â€” itâ€™s allowed, but not portable or predictable.

## 1.1 Counting Distinct Values in a Column

We can combine `DISTINCT` with combinations of values like `count()`. This will return a count of distinct values from a column.

```{python}
%%sql
SELECT count(libname)
FROM libraries_2017;
```

```{python}
%%sql
SELECT count(DISTINCT libname)
FROM libraries_2017;
```

---

Removing duplicates reduces the number of library names. It happens that some libraries share their names with other agencies. We will see how we can see the duplicates soon.

## 1.2 Finding Maximum and Minimum Values using max() and min()

The `max()` and `min()` funtions return the largest and smallest values in a column. In our case, we will be using them to detect some annomalies in the data, and to get a sense of the scope of the values reported. Let's check the number of annual visits to the library agencies for 2018.

```{python}
%%sql
SELECT max(visits), min(visits)
FROM libraries_2018;
```

---

The value of -3 is unexpected. It happens that the negative values for this database indicate "not applicable", used when a library has closed temporarily or permanently. This is a terrible way of coding missing data, but we can cope with that.

We could use `WHERE min(visits > 0)` to guarantee that only positive numbers are in the game.

A better way of handling these situations would be using NULL values and adding a column to hold text explaining why the value is NULL.

## 1.3 Aggregating with GROUP BY

